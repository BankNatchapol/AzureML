# -*- coding: utf-8 -*-
"""22p21c0039_W4HW1_Natchapol_12102020.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x6KT0CHpD6LebpbiG3bVFCKKNDuLqNLr
"""

import warnings
warnings.filterwarnings("ignore",category=UserWarning)   

import os
from os import listdir
from os.path import isfile, join
os.makedirs('outputs', exist_ok=True)

import torch

import torch.nn as nn
import torch.nn.functional as F

import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

import torchvision
import torchvision.models as models
import torchvision.transforms as transforms

import skorch
from skorch.callbacks import Checkpoint, LRScheduler, EpochScoring
from skorch.classifier import NeuralNetClassifier 

from sklearn.model_selection import cross_val_predict
from skimage.filters import threshold_otsu
from skimage.util import invert, img_as_ubyte, img_as_bool
from skimage.morphology import  binary_closing, thin

from sklearn.metrics import  mean_absolute_error
import cv2

import PIL
from PIL import Image
import pandas as pd
import numpy as np

from azureml.core import Run
import argparse

run = Run.get_context()

parser = argparse.ArgumentParser()
parser.add_argument('--data_path', type=str, help='Path to the training data')
args = parser.parse_args()
print("===== DATA =====")
print("DATA PATH: " + args.data_path)
print("LIST FILES IN DATA PATH...")
print(os.listdir(args.data_path))
print("================")

df = pd.read_csv(f'{args.data_path}/data/mnist.train.map.csv')

drop_list = pd.read_csv(f'{args.data_path}/data/drop_lists.csv',header=None)[:-1] \
            .rename({0:'id',1:'category'},axis=1)

index_dropped = list(df[df['id'].isin(drop_list['id'])].index)
dropped_df = df.drop((index_dropped), inplace = False).reset_index()

msk = np.random.rand(len(dropped_df)) < 0.9
train = dropped_df[msk]
test = dropped_df[~msk]

train.to_csv('./outputs/train_map.csv')
test.to_csv('./outputs/train_test_map.csv')
print('Downloaded trainset')

class THMNISTDataset(Dataset):
    def __init__(self, csv_file, id_col, target_col, root_dir, 
                 sufix=None, transform=None):
      
        self.data      = pd.read_csv(csv_file)
        self.id        = id_col
        self.target    = target_col
        self.root      = root_dir
        self.sufix     = sufix
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_name = self.data.loc[idx, self.id]
        if self.sufix is not None:
            img_name = img_name + self.sufix
        image = Image.open(os.path.join(self.root, img_name))
        if self.transform is not None:
            image = self.transform(image)
        #print(image.shape)
        label = self.data.loc[idx, self.target]
        return image[:3], label

class Erode(object):
    def __init__(self):
        self.kernel = np.ones((5,5),np.uint8)

    def __call__(self, sample):
        image = sample[0].numpy()
        #print(image.shape)
        erosion = cv2.erode(image,self.kernel,iterations = 1)
        return  torch.from_numpy(erosion)

def crop_image(img,tol=0):
    # img is 2D image data
    # tol  is tolerance
    mask = img<tol
    return img[np.ix_(mask.any(1),mask.any(0))]

class Crop(object):
  def __call__(self, sample):
        image = sample.numpy()
        croped = crop_image(image, 1)
        return  torch.from_numpy(croped[None,None,:,:])

def get_binary(img):    
    thresh = threshold_otsu(img)
    binary = img > thresh
    return binary

def to_thin(fn):    
    im = img_as_bool(get_binary(invert(fn)))
    out = binary_closing(thin(im)).astype(np.float32)
    return out[None,:,:]

BATCH_SIZE = 128

train_transform = transforms.Compose([
                                  transforms.Grayscale(num_output_channels=1),  
                                  transforms.RandomAffine(10,shear=10,fillcolor=(255, )),  
                                  transforms.ToTensor(),
                                  Erode(),
                                  Crop(),
                                  transforms.Lambda(lambda x: F.interpolate(x, size=(128,128))),   
                                  transforms.Lambda(lambda x:  to_thin(np.array(x[0][0]))),   
                                  transforms.Lambda(lambda x: np.repeat(x,3, axis=0)),
                                  ]) 
test_transform = transforms.Compose([
                                  transforms.Grayscale(num_output_channels=1),  
                                  transforms.ToTensor(),
                                  Erode(),
                                  Crop(),
                                  transforms.Lambda(lambda x: F.interpolate(x, size=(128,128))),
                                   transforms.Lambda(lambda x:  to_thin(np.array(x[0][0]))),
                                  transforms.Lambda(lambda x: np.repeat(x,3, axis=0)),
                                  ]) 

train_data_path = f'{args.data_path}/data/train'


trainset = THMNISTDataset('./outputs/train_map.csv','id','category',train_data_path,transform=train_transform)

testset = THMNISTDataset('./outputs/train_test_map.csv','id','category',train_data_path,transform=test_transform)

trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

print('Created trainset')

def MAE(net, X, y):
    ypred = net.predict(X)
    loss = net.history[:, 'train_loss'][-1]
    acc = net.history[:, 'valid_acc'][-1]
    mae = mean_absolute_error(y, ypred)
    
    run.log('loss', loss)
    run.log('acc', acc)
    run.log('mae', mae)
    return mae

def train(name, PATH, model, trainset, testset, criterion, optimizer,lr=1e-3 ,n_epochs=2):
    net = NeuralNetClassifier(
                                module=model,
                                criterion = criterion,
                                max_epochs=n_epochs,
                                lr=lr,
                                optimizer=optimizer,
                                device=dev,  
                                train_split=skorch.dataset.CVSplit(5),
                                callbacks = [Checkpoint(monitor='valid_acc_best', f_params="./outputs/"+PATH+".pt"), 
                                             LRScheduler(), EpochScoring(MAE)]                                
                              )
    net.fit(trainset, y=None)
    return net

def split_XY(dataset):
    l = []
    a = torch.Tensor(len(dataset), 3, 128, 128)
    for i, (image, label) in enumerate(dataset,0):

        
        a[i, :, :, :] = torch.from_numpy(image)
        l.append(label)
        if i == len(dataset)-1:
          break
    return a, torch.tensor(l, dtype=torch.long)

def test(model, testset, n):
    x, y = split_XY(testset)
    y_pred = cross_val_predict(model, x, y, cv=5)
    return  y, y_pred

if torch.cuda.is_available():  
    dev = "cuda:0" 
else:  
    dev = "cpu"
print("Dev : ",dev)

googlenet = models.googlenet(pretrained=True).to(dev) 
print('Defined model\n')
criterion = nn.CrossEntropyLoss
optimizer = optim.SGD

n_epochs = 10
print('start training\n')
print('googlenet ---------------------------')
Googlenet = train('Googlenet', 'googlenetCheckPoint',googlenet, trainset, testset, criterion, optimizer, n_epochs=n_epochs)

googlenet = models.googlenet(pretrained=True).to(dev) 

Googlenet = NeuralNetClassifier(
                                module=googlenet,
                                criterion = criterion,
                                max_epochs=n_epochs,
                                lr=1e-3,
                                optimizer=optimizer,
                                device=dev,  
                                train_split=skorch.dataset.CVSplit(5)                                                 
                              ).initialize()
                              
Googlenet.load_params(f_params='./outputs/googlenetCheckPoint.pt')
print('Finished Training')

y ,ypred = test(Googlenet, testset, len(testset))
print("googlenet mae: ", round(mean_absolute_error(y, ypred),5))

files = [f for f in listdir(f'{args.data_path}/data/test') if isfile(join(f'{args.data_path}/data/test', f))]
test_map = pd.DataFrame({'id':files,'category':'no'})
test_map.to_csv('./outputs/test_map.csv')

dataset = THMNISTDataset('./outputs/test_map.csv','id','category', f'{args.data_path}/data/test',transform=test_transform)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

preds = list(Googlenet.predict(dataset))

preddf = pd.DataFrame({'id':files,'category':preds })
train_map = pd.read_csv(f'{args.data_path}/data/mnist.train.map.csv')

train_rules = pd.read_csv(f'{args.data_path}/data/train.rules.csv')

print('Downloaded rules')

def isnan(value):
    try:
        return np.isnan(value)
    except:
        return False

def solver(row):
  f1 = row['feature1']
  f2 = row['feature2']
  f3 = row['feature3']
  #print(type(f1))
  if isnan(f1):
    return f2 + f3
  elif f1 == 0.0:
    return f2*f3
  elif f1 == 1.0:
    return abs(f2-f3)
  elif f1 == 2.0:
    return (f2 + f3)*abs(f2 - f3)
  elif f1 == 3.0:
    return  abs((f3*(f3+1) - f2*(f2-1))/2)
  elif f1 == 4.0:
    return 50 + (f2-f3)
  elif f1 == 5.0:
    return min(f2,f3)
  elif f1 == 6.0:
    return max(f2,f3)
  elif f1 == 7.0:
    return ((f2 * f3)%9) * 11
  elif f1 == 8.0:
    return (((f2 **2) + 1) * f2 + f3*(f3 +1) ) %99
  elif f1 == 9.0:
    return 50 + f2
  else:
    return 0

fe = pd.merge(pd.merge(pd.merge(train_rules, train_map, left_on='feature1', right_on='id', how='left',suffixes=('_left','_right')), 
                                             train_map, left_on='feature2', right_on='id', how='left',suffixes=('_left_left','_right_right')), 
                                             train_map, left_on='feature3', right_on='id', how='left',suffixes=('_left_left_left','_right_right_right'))[['id_left', 'category_left_left', 'category_right_right', 'category','predict']] \
                                            .rename(columns = {'id_left':'id','category_left_left':'feature1','category_right_right':'feature2','category':'feature3'}, inplace = False)

train_result = pd.concat([fe,pd.DataFrame({'predict_predict':fe.apply(solver, axis=1)})],axis=1)
mean_absolute_error(train_result['predict'], train_result['predict_predict'] )

test_rules = pd.read_csv(f'{args.data_path}/data/test.rules.csv')

lastrule = pd.merge(pd.merge(pd.merge(test_rules,preddf, left_on='feature1',right_on='id', how='left' ,suffixes=('_left','_right')) ,
                                                 preddf, left_on='feature2', right_on='id', how='left') ,
                                                 preddf, left_on='feature3', right_on='id', how='left')[['id_left','category_x','category_y','category']] \
                                                .rename(columns = {'id_left':'id','category_x':'feature1','category_y':'feature2','category':'feature3'}, inplace = False)

test_result = pd.concat([lastrule,pd.DataFrame({'predict':lastrule.apply(solver, axis=1)})],axis=1)

submit = test_result[['id','predict']]
submit.to_csv('./outputs/submit.csv', index=False)


